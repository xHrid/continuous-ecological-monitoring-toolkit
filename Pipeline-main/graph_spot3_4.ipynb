{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a72b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, entropy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Load Combined Detections Data\n",
    "# 'birdnet_classification.csv' file should contain birdnet prediction data.\n",
    "# results_df should have the combined data of all the spots.\n",
    "OUTPUTS = [r\"E:\\processed_data\\acoustic_biodiversity\\analysis\\audio_raw_spot_1_original_spot_20072025-29072025_2R4W_birdnet_classification.csv\", r\"E:\\processed_data\\acoustic_biodiversity\\analysis\\audio_raw_spot_2_peacock_spot_20072025-03082025_2R4W_birdnet_classification.csv\", r\"E:\\processed_data\\acoustic_biodiversity\\analysis\\audio_raw_spot_3_investigation_spot_20072025-03082025_2R4W_birdnet_classification.csv\" , r\"E:\\processed_data\\acoustic_biodiversity\\analysis\\audio_raw_spot_4_yoga_spot_20072025-03082025_2R4W_birdnet_classification.csv\"]\n",
    "results_df = pd.DataFrame()\n",
    "for i in OUTPUTS:\n",
    "    try:\n",
    "        df = pd.read_csv(i)\n",
    "        results_df = pd.concat([results_df, df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {i} not found. Skipping...\")\n",
    "\n",
    "results_df['filename'] = results_df['filename'].str.replace(\"STOP\", \"SPOT\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Essential Preprocessing - Add Spot and Date Information\n",
    "# This step extracts location ('Spot') and time ('Date') from the filenames,\n",
    "# enabling all subsequent spatial and temporal analyses.\n",
    "\n",
    "results_df['Spot'] = results_df['filename'].str.extract(r'(SPOT\\d+)', expand=False).str.lower().str.replace('spot', 'spot_')\n",
    "\n",
    "\n",
    "date_info = results_df['filename'].str.extract(r'_(\\d{8})_')\n",
    "results_df['Date'] = pd.to_datetime(date_info[0], format='%Y%m%d')\n",
    "\n",
    "results_df.dropna(subset=['Spot', 'Date'], inplace=True)\n",
    "\n",
    "print(\"Preprocessing complete. 'Spot' and 'Date' columns added.\")\n",
    "print(f\"Spots found: {sorted(results_df['Spot'].unique())}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_stickiness",
   "metadata": {},
   "source": [
    "### Part 3: Behavioral Stickiness Analyses\n",
    "This section quantifies the predictability of species behavior. \n",
    "- **Temporal Stickiness:** How consistent is a species' daily activity schedule?\n",
    "- **Spatial Stickiness:** How consistently does a species prefer the same locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442af756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal Stickiness Analysis ---\n",
    "print(\"Calculating Temporal Stickiness...\")\n",
    "\n",
    "date_info = results_df['filename'].str.extract(r'_(\\d{8})_')\n",
    "results_df['Date'] = pd.to_datetime(date_info[0], format='%Y%m%d')\n",
    "results_df['Spot'] = results_df['filename'].str.extract(r'(SPOT\\d+)', expand=False).str.lower().str.replace('spot', 'spot_')\n",
    "results_df.dropna(subset=['Spot', 'Date'], inplace=True)\n",
    "\n",
    "ACTIVITY_HOURS = range(5, 20)\n",
    "activity_df = results_df[(results_df['confidence'] >= 0.5) & (results_df['hour'].isin(ACTIVITY_HOURS))].copy()\n",
    "\n",
    "species_list = activity_df['label'].unique()\n",
    "spot_list = activity_df['Spot'].unique()\n",
    "date_list = sorted(activity_df['Date'].unique())\n",
    "num_days = activity_df['Date'].nunique() # Calculate the number of unique days for averaging\n",
    "\n",
    "temporal_stickiness = {}\n",
    "\n",
    "for species in species_list:\n",
    "    species_spot_correlations = []\n",
    "    for spot in spot_list:\n",
    "        spot_day_correlations = []\n",
    "        for i in range(len(date_list) - 1):\n",
    "            day_k = date_list[i]\n",
    "            day_k_plus_1 = date_list[i+1]\n",
    "            series_k = activity_df[(activity_df['label'] == species) & (activity_df['Spot'] == spot) & (activity_df['Date'] == day_k)]['hour'].value_counts().reindex(ACTIVITY_HOURS, fill_value=0)\n",
    "            series_k_plus_1 = activity_df[(activity_df['label'] == species) & (activity_df['Spot'] == spot) & (activity_df['Date'] == day_k_plus_1)]['hour'].value_counts().reindex(ACTIVITY_HOURS, fill_value=0)\n",
    "            if series_k.sum() > 0 and series_k_plus_1.sum() > 0:\n",
    "                corr, _ = spearmanr(series_k, series_k_plus_1)\n",
    "                spot_day_correlations.append(corr)\n",
    "        if spot_day_correlations:\n",
    "            species_spot_correlations.append(np.mean(spot_day_correlations))\n",
    "    if species_spot_correlations:\n",
    "        temporal_stickiness[species] = np.mean(species_spot_correlations)\n",
    "\n",
    "temporal_stickiness_df = pd.DataFrame(list(temporal_stickiness.items()), columns=['label', 'Temporal_Stickiness'])\n",
    "top_temporal = temporal_stickiness_df.sort_values(by='Temporal_Stickiness', ascending=False).head(40)\n",
    "\n",
    "\n",
    "# --- 2. Calculate Average Daily Calls for the Top Species ---\n",
    "print(\"Calculating Average Daily Calls for top species...\")\n",
    "# Filter the main dataframe to only the species in the top_temporal chart\n",
    "top_species_calls_df = activity_df[activity_df[\"label\"].isin(top_temporal[\"label\"])]\n",
    "\n",
    "# Calculate the average by counting total calls and dividing by the number of days\n",
    "avg_calls_per_day = top_species_calls_df.groupby(\"label\").size().reset_index(name=\"total_calls\")\n",
    "avg_calls_per_day[\"Avg_Calls_Per_Day\"] = avg_calls_per_day[\"total_calls\"] / num_days\n",
    "\n",
    "# **Important**: Reorder the data to match the stickiness chart's order\n",
    "avg_calls_per_day = avg_calls_per_day.set_index(\"label\").reindex(top_temporal[\"label\"]).fillna(0).reset_index()\n",
    "\n",
    "\n",
    "# --- 3. Create Side-by-Side Visualization ---\n",
    "print(\"Generating plots...\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 10))\n",
    "fig.suptitle('Temporal Stickiness vs. Average Daily Call Volume', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Temporal Stickiness\n",
    "sns.barplot(x='Temporal_Stickiness', y='label', data=top_temporal, palette='plasma', ax=ax1)\n",
    "ax1.set_title('Top 40 Species by Stickiness (Predictability)', fontsize=14)\n",
    "ax1.set_xlabel(\"Average Spearman Correlation (ρ)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Species\", fontsize=12)\n",
    "ax1.set_xlim(-0.2, 1.0)\n",
    "ax1.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Average Daily Calls\n",
    "sns.barplot(x=\"Avg_Calls_Per_Day\", y=\"label\", data=avg_calls_per_day, palette=\"magma\", ax=ax2)\n",
    "ax2.set_title('Average Daily Call Volume of Top Species', fontsize=14)\n",
    "ax2.set_xlabel(\"Average Calls per Day\", fontsize=12)\n",
    "ax2.set_ylabel(\"\") # Hide the y-label to avoid repetition\n",
    "ax2.tick_params(axis='y', which='both', left=False, labelleft=False) # Hide y-axis ticks and labels\n",
    "\n",
    "# Add connecting lines to make comparison easier\n",
    "for i in range(len(top_temporal)):\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.axhline(y=i, color='grey', linestyle='--', alpha=0.5, linewidth=1, zorder=0)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_temporal_stickiness_cell",
   "metadata": {
    "tags": [
     "new-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Temporal-Spatial Stickiness Analysis ---\n",
    "print(\"Calculating Temporal-Spatial Stickiness...\")\n",
    "\n",
    "# --- Combined Analysis and Visualization Loop ---\n",
    "print(\"Starting combined analysis for all spots...\")\n",
    "\n",
    "for j, filepath in enumerate(OUTPUTS):\n",
    "\n",
    "    # --- 1. Data Loading and Preprocessing ---\n",
    "    results = pd.read_csv(filepath)\n",
    "    results['filename'] = results['filename'].str.replace(\"STOP\", \"SPOT\", regex=False)\n",
    "    date_info = results['filename'].str.extract(r'_(\\d{8})_')\n",
    "    results['Date'] = pd.to_datetime(date_info[0], format='%Y%m%d')\n",
    "    results['Spot'] = results['filename'].str.extract(r'(SPOT\\d+)', expand=False).str.lower().str.replace('spot', 'spot_')\n",
    "    results.dropna(subset=['Spot', 'Date'], inplace=True)\n",
    "    \n",
    "    ACTIVITY_HOURS = range(5, 20)\n",
    "    activity_df = results[(results['confidence'] >= 0.5) & (results['hour'].isin(ACTIVITY_HOURS))].copy()\n",
    "\n",
    "    species_list = activity_df['label'].unique()\n",
    "    spot_list = activity_df['Spot'].unique()\n",
    "    date_list = sorted(activity_df['Date'].unique())\n",
    "    num_days = activity_df['Date'].nunique() # Get total number of unique days for averaging\n",
    "\n",
    "    # --- 2. Calculate Temporal-Spatial Stickiness ---\n",
    "    temporal_stickiness = {}\n",
    "    for species in species_list:\n",
    "        species_spot_correlations = []\n",
    "        for spot in spot_list:\n",
    "            spot_day_correlations = []\n",
    "            for i in range(len(date_list) - 1):\n",
    "                day_k = date_list[i]\n",
    "                day_k_plus_1 = date_list[i+1]\n",
    "                series_k = activity_df[(activity_df['label'] == species) & (activity_df['Spot'] == spot) & (activity_df['Date'] == day_k)]['hour'].value_counts().reindex(ACTIVITY_HOURS, fill_value=0)\n",
    "                series_k_plus_1 = activity_df[(activity_df['label'] == species) & (activity_df['Spot'] == spot) & (activity_df['Date'] == day_k_plus_1)]['hour'].value_counts().reindex(ACTIVITY_HOURS, fill_value=0)\n",
    "                if series_k.sum() > 0 and series_k_plus_1.sum() > 0:\n",
    "                    corr, _ = spearmanr(series_k, series_k_plus_1)\n",
    "                    spot_day_correlations.append(corr)\n",
    "            if spot_day_correlations:\n",
    "                species_spot_correlations.append(np.mean(spot_day_correlations))\n",
    "        if species_spot_correlations:\n",
    "            temporal_stickiness[species] = np.mean(species_spot_correlations)\n",
    "\n",
    "    temporal_stickiness_df = pd.DataFrame(list(temporal_stickiness.items()), columns=['label', 'Temporal_Stickiness'])\n",
    "    top_temporal = temporal_stickiness_df.sort_values(by='Temporal_Stickiness', ascending=False).head(40)\n",
    "\n",
    "    # --- 3. Calculate Average Daily Calls for the Top Species ---\n",
    "    top_species_calls_df = activity_df[activity_df[\"label\"].isin(top_temporal[\"label\"])]\n",
    "    \n",
    "    avg_calls_per_day = top_species_calls_df.groupby(\"label\").size().reset_index(name=\"total_calls\")\n",
    "    avg_calls_per_day[\"Avg_Calls_Per_Day\"] = avg_calls_per_day[\"total_calls\"] / num_days\n",
    "    \n",
    "    avg_calls_per_day = avg_calls_per_day.set_index(\"label\").reindex(top_temporal[\"label\"]).fillna(0).reset_index()\n",
    "\n",
    "    # --- 4. Create Side-by-Side Plots ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 10))\n",
    "    fig.suptitle(f'Analysis for Spot {j+1}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Temporal Stickiness\n",
    "    sns.barplot(x='Temporal_Stickiness', y='label', data=top_temporal, palette='plasma', ax=ax1)\n",
    "    ax1.set_title('Temporal-Spatial Stickiness (Predictability)', fontsize=14)\n",
    "    ax1.set_xlabel(\"Average Spearman Correlation (ρ)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Species\", fontsize=12)\n",
    "    ax1.set_xlim(-0.2, 1.0)\n",
    "    ax1.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot 2: Average Daily Calls\n",
    "    sns.barplot(x=\"Avg_Calls_Per_Day\", y=\"label\", data=avg_calls_per_day, palette=\"magma\", ax=ax2)\n",
    "    ax2.set_title('Average Daily Call Volume', fontsize=14)\n",
    "    ax2.set_xlabel(\"Average Calls per Day\", fontsize=12)\n",
    "    ax2.set_ylabel(\"\") \n",
    "    ax2.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
    "\n",
    "    # --- 5. Add Connecting Lines for Readability ---\n",
    "    for i in range(len(top_temporal)):\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.axhline(\n",
    "                y=i,\n",
    "                color='grey',\n",
    "                linestyle='--',\n",
    "                alpha=0.5,\n",
    "                linewidth=1,\n",
    "                zorder=0  # Draw lines behind the bars\n",
    "            )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spatial Stickiness Analysis ---\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "print(\"\\nCalculating Spatial Stickiness...\")\n",
    "\n",
    "# Filter for high-confidence results and create a copy to avoid SettingWithCopyWarning\n",
    "spatial_df = results_df[results_df['confidence'] >= 0.5].copy()\n",
    "\n",
    "# Get unique lists for species, spots, and dates\n",
    "species_list = spatial_df['label'].unique()\n",
    "spot_list = sorted(spatial_df['Spot'].unique())\n",
    "date_list = sorted(spatial_df['Date'].unique())\n",
    "\n",
    "\n",
    "# --- 2. Calculate Spatial Stickiness ---\n",
    "\n",
    "# Check if there is data from at least two spots, which is required for correlation\n",
    "if len(spot_list) < 2:\n",
    "    print(\"\\nERROR: Spatial stickiness requires data from at least 2 spots.\")\n",
    "else:\n",
    "    spatial_stickiness = {}\n",
    "\n",
    "    # Iterate over each species to calculate its stickiness score\n",
    "    for species in species_list:\n",
    "        daily_rank_correlations = []\n",
    "\n",
    "        # Compare the spatial distribution of calls between consecutive days (k and k+1)\n",
    "        for i in range(len(date_list) - 1):\n",
    "            day_k = date_list[i]\n",
    "            day_k_plus_1 = date_list[i + 1]\n",
    "\n",
    "            # Get call counts per spot for day k, reindexing to ensure all spots are included\n",
    "            counts_k = (\n",
    "                spatial_df[(spatial_df['label'] == species) & (spatial_df['Date'] == day_k)]\n",
    "                .groupby('Spot')\n",
    "                .size()\n",
    "                .reindex(spot_list, fill_value=0)\n",
    "            )\n",
    "            # Get call counts per spot for day k+1\n",
    "            counts_k_plus_1 = (\n",
    "                spatial_df[(spatial_df['label'] == species) & (spatial_df['Date'] == day_k_plus_1)]\n",
    "                .groupby('Spot')\n",
    "                .size()\n",
    "                .reindex(spot_list, fill_value=0)\n",
    "            )\n",
    "\n",
    "            # Correlation can only be calculated if there is variance in the data\n",
    "            # (i.e., the counts are not constant across all spots)\n",
    "            if counts_k.nunique() > 1 and counts_k_plus_1.nunique() > 1:\n",
    "                corr, _ = spearmanr(counts_k, counts_k_plus_1)\n",
    "                \n",
    "                # Append the valid correlation score\n",
    "                if not np.isnan(corr):\n",
    "                    daily_rank_correlations.append(corr)\n",
    "\n",
    "        # If any valid correlations were found, calculate the average for the species\n",
    "        if daily_rank_correlations:\n",
    "            spatial_stickiness[species] = np.mean(daily_rank_correlations)\n",
    "\n",
    "    # --- 3. Create and Sort the Results DataFrame ---\n",
    "    spatial_stickiness_df = pd.DataFrame(\n",
    "        list(spatial_stickiness.items()),\n",
    "        columns=['label', 'Spatial_Stickiness']\n",
    "    )\n",
    "    spatial_stickiness_df = spatial_stickiness_df.sort_values(\n",
    "        by='Spatial_Stickiness', ascending=False\n",
    "    )\n",
    "\n",
    "    # --- Visualization for Spatial Stickiness in 2 parts ---\n",
    "    top_spatial = spatial_stickiness_df.head(80)\n",
    "\n",
    "    # Split into two halves\n",
    "    top40 = top_spatial.head(40)\n",
    "    next40 = top_spatial.iloc[40:80]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 12), sharex=True)\n",
    "\n",
    "    # First 40\n",
    "    sns.barplot(\n",
    "        x='Spatial_Stickiness', y='label', \n",
    "        data=top40, palette='viridis', ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(\"Top 40 Species by Spatial Stickiness\", fontsize=14)\n",
    "    axes[0].set_xlabel(\"Average Spearman Correlation (ρ)\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Species\", fontsize=12)\n",
    "    axes[0].set_xlim(-0.2, 1.0)\n",
    "    axes[0].tick_params(axis='y', labelsize=11)\n",
    "    axes[0].grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Next 40\n",
    "    sns.barplot(\n",
    "        x='Spatial_Stickiness', y='label', \n",
    "        data=next40, palette='viridis', ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"Next 40 Species by Spatial Stickiness\", fontsize=14)\n",
    "    axes[1].set_xlabel(\"Average Spearman Correlation (ρ)\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Species\", fontsize=12)\n",
    "    axes[1].set_xlim(-0.2, 1.0)\n",
    "    axes[1].tick_params(axis='y', labelsize=11)\n",
    "    axes[1].grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_spatial_stickiness_cell",
   "metadata": {
    "tags": [
     "new-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# -- Visualization for Activity Heatmap (Aligned with Spatial Stickiness) --\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "print(\"Preparing data for the activity heatmap...\")\n",
    "activity_df = results_df[results_df['confidence'] >= 0.5].copy()\n",
    "daily_counts = activity_df.groupby(['label', 'Spot', 'Date']).size().reset_index(name='daily_calls')\n",
    "avg_calls_per_day = daily_counts.groupby(['label', 'Spot'])['daily_calls'].mean().reset_index()\n",
    "\n",
    "# --- 2. Pivot Data  ---\n",
    "heatmap_data = avg_calls_per_day.pivot_table(\n",
    "    index='label',\n",
    "    columns='Spot',\n",
    "    values='daily_calls',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# --- 3. MODIFIED: Align Heatmap Data with Stickiness Plot Order ---\n",
    "\n",
    "print(\"Aligning heatmap species order with the spatial stickiness plot...\")\n",
    "\n",
    "# Get the exact list of species in the order they appear in the stickiness plot\n",
    "species_order_from_stickiness = top_spatial['label'].tolist()\n",
    "\n",
    "# Filter and reorder the heatmap data to match this specific list and order.\n",
    "# .reindex() ensures all species from the stickiness plot are included.\n",
    "aligned_heatmap_data = heatmap_data.reindex(species_order_from_stickiness).fillna(0)\n",
    "\n",
    "# Apply log transformation to the newly aligned data\n",
    "log_heatmap_data = np.log1p(aligned_heatmap_data)\n",
    "\n",
    "\n",
    "# --- 4. Visualization (Using the aligned data) ---\n",
    "print(\"Generating final aligned heatmap...\")\n",
    "\n",
    "# The data is already filtered to the top 80, so we just split it for plotting\n",
    "if len(log_heatmap_data) > 40:\n",
    "    # Split the aligned data into the two halves\n",
    "    heatmap_part1 = log_heatmap_data.iloc[0:40]\n",
    "    heatmap_part2 = log_heatmap_data.iloc[40:80]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 12))\n",
    "    \n",
    "    # --- Plot Top 40 (Left) ---\n",
    "    sns.heatmap(\n",
    "        heatmap_part1, cmap='YlGnBu', linewidths=0.5, linecolor='black',\n",
    "        annot=False, cbar=False, ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Bird Activity (Top 40 by Stickiness)', fontsize=14)\n",
    "    axes[0].set_xlabel('Spot', fontsize=12)\n",
    "    axes[0].set_ylabel('Species (Sorted by Spatial Stickiness)', fontsize=12)\n",
    "    axes[0].tick_params(axis='x', rotation=0, labelsize=10)\n",
    "    axes[0].tick_params(axis='y', labelsize=9)\n",
    "\n",
    "    # --- Plot Next 40 (Right) ---\n",
    "    cbar_ax = fig.add_axes([.94, .25, .015, .5])\n",
    "    sns.heatmap(\n",
    "        heatmap_part2, cmap='YlGnBu', linewidths=0.5, linecolor='black',\n",
    "        annot=False, cbar_ax=cbar_ax,\n",
    "        cbar_kws={'label': 'Log(Avg Calls per Day + 1)'}, ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title('Bird Activity (Next 40 by Stickiness)', fontsize=14)\n",
    "    axes[1].set_xlabel('Spot', fontsize=12)\n",
    "    axes[1].set_ylabel('', fontsize=12)\n",
    "    axes[1].tick_params(axis='x', rotation=0, labelsize=10)\n",
    "    axes[1].tick_params(axis='y', labelsize=9)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1, right=0.9, wspace=1.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_heatmaps",
   "metadata": {},
   "source": [
    "### Part 4: Spot-wise Bird Activity Heatmaps\n",
    "\n",
    "**How to Read These Heatmaps:**\n",
    "\n",
    "Each chart below represents a single monitoring spot and serves as its unique acoustic \"fingerprint.\" Here’s how to interpret it:\n",
    "\n",
    "-   **Y-Axis (Bird Species):** Shows the top 25 most frequently detected species at that specific spot.\n",
    "-   **X-Axis (Hour of Day):** Represents the 24 hours of the day.\n",
    "-   **Color and Numbers:** The color of each cell—and the number inside it—represents the **average number of detections for that species during that hour**, averaged across all days of recording. Darker colors indicate higher average activity.\n",
    "\n",
    "**What to Look For:**\n",
    "\n",
    "1.  **Horizontal Patterns (Across Hours):** Look for rows with dark bands. These show a species' daily schedule. A dark band between hours 5-8 indicates a strong dawn chorus. A second, smaller peak in the evening might also be visible.\n",
    "2.  **Vertical Patterns (Across Species):** Look for columns with many dark cells. These are the peak activity hours for the entire avian community at that spot (typically dawn).\n",
    "3.  **Comparing Charts:** By comparing the heatmap for Spot 1 to Spot 4, you can see which species are dominant in each location and how their daily patterns might differ based on the habitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_heatmap_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Non-Normalized Heat Maps ---\n",
    "print(\"\\nGenerating Non-Normalized Spot-wise Activity Heatmaps...\")\n",
    "\n",
    "activity_df = results_df[results_df['confidence'] >= 0.5].copy()\n",
    "activity_df = activity_df[~activity_df['label'].str.contains(\"Engine|Siren\", na=False)]\n",
    "\n",
    "for spot in sorted(activity_df['Spot'].unique()):\n",
    "    spot_df = activity_df[activity_df['Spot'] == spot]\n",
    "    num_days = spot_df['Date'].nunique()\n",
    "    if num_days == 0:\n",
    "        continue\n",
    "\n",
    "    top_species_in_spot = spot_df['label'].value_counts().nlargest(25).index\n",
    "    spot_df_top = spot_df[spot_df['label'].isin(top_species_in_spot)]\n",
    "\n",
    "    activity_pivot = spot_df_top.pivot_table(\n",
    "        index='label', \n",
    "        columns='hour', \n",
    "        values='filename', \n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    average_activity_pivot = activity_pivot / num_days\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(\n",
    "        average_activity_pivot, \n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=.5,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={'label': 'Avg. Detections per Hour'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Average Detections per Hour for {spot.replace('_', ' ').title()} (Averaged over {num_days} days)\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"label\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Normalized Heat Maps ---\n",
    "print(\"\\nGenerating Normalized  Spot-wise Activity Heatmaps...\")\n",
    "\n",
    "activity_df = results_df[results_df['confidence'] >= 0.5].copy()\n",
    "activity_df = activity_df[~activity_df['label'].str.contains(\"Engine|Siren\", na=False)]\n",
    "\n",
    "for spot in sorted(activity_df['Spot'].unique()):\n",
    "    spot_df = activity_df[activity_df['Spot'] == spot]\n",
    "    num_days = spot_df['Date'].nunique()\n",
    "    if num_days == 0:\n",
    "        continue\n",
    "\n",
    "    top_species_in_spot = spot_df['label'].value_counts().nlargest(25).index\n",
    "    spot_df_top = spot_df[spot_df['label'].isin(top_species_in_spot)]\n",
    "\n",
    "    activity_pivot = spot_df_top.pivot_table(\n",
    "        index='label', \n",
    "        columns='hour', \n",
    "        values='filename', \n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    average_activity_pivot = activity_pivot / num_days\n",
    "    \n",
    "    # Row-wise normalization by sum\n",
    "    average_activity_pivot_norm = average_activity_pivot.div(\n",
    "        average_activity_pivot.sum(axis=1), axis=0\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(\n",
    "        average_activity_pivot_norm, \n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=.5,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={'label': 'Proportion of Daily Activity'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Normalized Hourly Activity for {spot.replace('_', ' ').title()} (Proportion per Species)\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"label\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_boxplots",
   "metadata": {},
   "source": [
    "### Part 5: Acoustic & Biodiversity Index Analysis\n",
    "This section first identifies the most effective acoustic indices by correlating them with a ground-truth biodiversity metric (Shannon Index). Then, it uses box plots to compare the distributions of these key indices across the different monitoring spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILES = [r\"E:\\monitoring_data\\sound_recordings\\spot_1_original_spot\\20072025-29072025_2R4W\\results.csv\", r\"E:\\monitoring_data\\sound_recordings\\spot_2_peacock_spot\\20072025-03082025_2R4W\\results.csv\", r\"E:\\monitoring_data\\sound_recordings\\spot_4_yoga_spot\\20072025-03082025_2R4W\\results.csv\", r\"E:\\monitoring_data\\sound_recordings\\spot_3_investigation_spot\\20072025-03082025_2R4W\\results.csv\"]\n",
    "SAVE = [] # paths to csv to save combined indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Preprocess Index Files ---\n",
    "results_i_df = pd.DataFrame()\n",
    "for i in INDEX_FILES:\n",
    "    try:\n",
    "        df = pd.read_csv(i)\n",
    "        results_i_df = pd.concat([results_i_df, df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {i} not found. Skipping...\")\n",
    "if 'filename' in df.columns:\n",
    "    # If it exists, rename it to 'Filename'\n",
    "    # The 'inplace=True' argument modifies the DataFrame directly.\n",
    "    df.rename(columns={'filename': 'Filename'}, inplace=True)\n",
    "    print(\"\\nSuccessfully renamed 'filename' to 'Filename'.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_i_df['Spot'] = results_i_df['Filename'].str.extract(r'(SPOT\\d+)', expand=False).str.lower().str.replace('spot', 'spot_')\n",
    "date_info = results_i_df['Filename'].str.extract(r'_(\\d{8})_')\n",
    "results_i_df['Date'] = pd.to_datetime(date_info[0], format='%Y%m%d')\n",
    "results_i_df.dropna(subset=['Spot'], inplace=True)\n",
    "\n",
    "print(\"Preprocessing complete. 'Spot' and 'Date' columns added.\")\n",
    "print(f\"Spots found: {sorted(results_i_df['Spot'].unique())}\")\n",
    "results_i_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79741026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper Functions (ensure these are defined in your script) ---\n",
    "def compute_shannon(labels):\n",
    "    \"\"\"Computes the Shannon diversity index from a series of labels.\"\"\"\n",
    "    counts = labels.value_counts()\n",
    "    proportions = counts / len(labels)\n",
    "    # Use np.log2 for the Shannon index calculation; add a small epsilon to avoid log(0)\n",
    "    proportions = proportions[proportions > 0]\n",
    "    shannon_index = -np.sum(proportions * np.log2(proportions))\n",
    "    return shannon_index\n",
    "\n",
    "def compute_simpson(labels):\n",
    "    \"\"\"Computes the Simpson diversity index from a series of labels.\"\"\"\n",
    "    counts = labels.value_counts()\n",
    "    n = len(labels)\n",
    "    if n < 2:\n",
    "        return np.nan # Simpson's index is undefined for less than 2 individuals\n",
    "    \n",
    "    numerator = np.sum(counts * (counts - 1))\n",
    "    denominator = n * (n - 1)\n",
    "    \n",
    "    # Simpson's Index (D) is numerator / denominator. We return 1 - D (Gini-Simpson index).\n",
    "    simpson_index = 1 - (numerator / denominator)\n",
    "    return simpson_index\n",
    "\n",
    "# --- Configuration: Define your file paths here ---\n",
    "# Assume OUTPUTS, INDEX_FILES, and SAVE are lists of file paths.\n",
    "# Example lists for testing:\n",
    "OUTPUTS = ['bird_detections.csv']\n",
    "INDEX_FILES = ['acoustic_indices.csv']\n",
    "SAVE = ['final_combined_indices.csv']\n",
    "\n",
    "# --- Create dummy CSV files for a runnable example ---\n",
    "# This part is just for demonstration. You would use your own files.\n",
    "pd.DataFrame({\n",
    "    'filename': ['audio1.wav', 'audio1.wav', 'audio2.wav', 'audio2.wav', 'audio2.wav', 'audio3.wav'],\n",
    "    'confidence': [0.9, 0.8, 0.4, 0.7, 0.9, 0.95],\n",
    "    'label': ['sparrow', 'robin', 'sparrow', 'finch', 'finch', 'sparrow'],\n",
    "    'Spot': [1, 1, 2, 2, 2, 3],\n",
    "    'Date': ['2023-10-26', '2023-10-26', '2023-10-27', '2023-10-27', '2023-10-27', '2023-10-28']\n",
    "}).to_csv('bird_detections.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Filename': ['audio1.wav', 'audio2.wav', 'audio4.wav'], # Note the capital 'F' and a non-matching file\n",
    "    'ADI': [0.5, 0.6, 0.7], 'ACI': [1.2, 1.3, 1.4], 'AEI': [0.8, 0.9, 1.0],\n",
    "    'NDSI': [0.1, 0.2, 0.3], 'MFC': [10, 11, 12], 'CLS': [2.5, 2.6, 2.7]\n",
    "}).to_csv('acoustic_indices.csv', index=False)\n",
    "# --- End of dummy file creation ---\n",
    "\n",
    "\n",
    "# Main processing loop\n",
    "for i in range(len(OUTPUTS)):\n",
    "    print(f\"--- Processing file set {i+1}: {OUTPUTS[i]} & {INDEX_FILES[i]} ---\")\n",
    "    combined_df = pd.DataFrame() # Initialize an empty DataFrame\n",
    "\n",
    "    try:\n",
    "        # 1. Load the two data files\n",
    "        detections_df = pd.read_csv(OUTPUTS[i])\n",
    "        acoustic_df = pd.read_csv(INDEX_FILES[i])\n",
    "\n",
    "        # 2. **Crucial Step:** Standardize column names to prevent merge errors.\n",
    "        # We rename 'Filename' (capital F) to 'filename' (lowercase f) if it exists.\n",
    "        if 'Filename' in detections_df.columns:\n",
    "            detections_df.rename(columns={'Filename': 'filename'}, inplace=True)\n",
    "        if 'Filename' in acoustic_df.columns:\n",
    "            acoustic_df.rename(columns={'Filename': 'filename'}, inplace=True)\n",
    "        print(\"Standardized 'filename' column for merging.\")\n",
    "\n",
    "        # 3. Calculate Shannon and Simpson indices in a single operation\n",
    "        print(\"Calculating Shannon and Simpson indices...\")\n",
    "        diversity_df = (\n",
    "            detections_df[detections_df['confidence'] >= 0.5]\n",
    "            .groupby('filename')\n",
    "            .agg(\n",
    "                Shannon=('label', compute_shannon),\n",
    "                Simpson=('label', compute_simpson),\n",
    "                Spot=('Spot', 'first'),\n",
    "                Date=('Date', 'first')\n",
    "            )\n",
    "            .reset_index()\n",
    "            .dropna(subset=['Shannon', 'Simpson'])\n",
    "        )\n",
    "\n",
    "        # 4. Aggregate acoustic indices (if they have multiple rows per file)\n",
    "        print(\"Aggregating acoustic indices...\")\n",
    "        avg_acoustic_df = (\n",
    "            acoustic_df.groupby('filename')\n",
    "            .agg({\n",
    "                'ADI': 'mean', 'ACI': 'mean', 'AEI': 'mean',\n",
    "                'NDSI': 'mean', 'MFC': 'mean', 'CLS': 'mean'\n",
    "            })\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # 5. Perform the final merge\n",
    "        print(\"Merging all computed data...\")\n",
    "        combined_df = pd.merge(\n",
    "            diversity_df,      # Contains Shannon, Simpson, Spot, Date\n",
    "            avg_acoustic_df,   # Contains all acoustic indices\n",
    "            on='filename',     # The common column to join on\n",
    "            how='inner'        # 'inner' ensures we only keep files present in BOTH dataframes\n",
    "        )\n",
    "        \n",
    "        if combined_df.empty:\n",
    "            print(\"\\nWARNING: Merge resulted in an empty DataFrame. No matching 'filename' entries found.\")\n",
    "        else:\n",
    "            print(\"\\n✅ Successfully merged all indices.\")\n",
    "            print(combined_df.head())\n",
    "\n",
    "    # 6. Improved error handling for clear feedback\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ ERROR: File not found. Please check your paths. Details: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"\\n❌ ERROR: A required column was not found. Please check your CSV files for column: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "\n",
    "    # 7. Save the final DataFrame to a CSV file\n",
    "    # This will save the merged data, or an empty file if an error occurred.\n",
    "    combined_df.to_csv(SAVE[i], index=False)\n",
    "    print(f\"\\nSaved results to {SAVE[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* if combined indices are available **********\n",
    "#pick the combined indices for any spot\n",
    "combined_indices_df = pd.read_csv(r\"E:\\processed_data\\acoustic_biodiversity\\analysis\\audio_raw_spot_4_yoga_spot_20072025-03082025_2R4W_combined_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_correlation_plot_cell",
   "metadata": {
    "tags": [
     "new-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Identify Best Acoustic Indices via Correlation Plot with shannon ---\n",
    "# This plot replicates the style from the research paper, showing which indices are most strongly\n",
    "# correlated with avian biodiversity (Shannon Index).\n",
    "\n",
    "def bootstrap_spearman(df, col_x, col_y, n_iterations=1000, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    r_values = []\n",
    "    for _ in range(n_iterations):\n",
    "        sample = df.sample(frac=1, replace=True)\n",
    "        r, _ = spearmanr(sample[col_x], sample[col_y])\n",
    "        r_values.append(r)\n",
    "    r_values = np.array(r_values)\n",
    "    return np.mean(r_values), np.percentile(r_values, 2.5), np.percentile(r_values, 97.5)\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    indices_to_test = ['ADI', 'ACI', 'AEI', 'NDSI', 'MFC', 'CLS']\n",
    "    corr_results = []\n",
    "    for index in indices_to_test:\n",
    "        mean_r, ci_lower, ci_upper = bootstrap_spearman(combined_indices_df, index, 'Shannon')\n",
    "        corr_results.append({'Index': index, 'Mean_r': mean_r, 'CI_lower': ci_lower, 'CI_upper': ci_upper})\n",
    "    \n",
    "    df_corr_results = pd.DataFrame(corr_results).sort_values('Mean_r')\n",
    "\n",
    "    # Plotting the correlation graph\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.errorbar(df_corr_results['Mean_r'], df_corr_results['Index'],\n",
    "                 xerr=[df_corr_results['Mean_r'] - df_corr_results['CI_lower'], \n",
    "                       df_corr_results['CI_upper'] - df_corr_results['Mean_r']],\n",
    "                 fmt='o', color='darkslateblue', capsize=5, elinewidth=2, markeredgewidth=2)\n",
    "\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.xlabel(\"Spearman's Rank Correlation (rs) with Shannon Index\", fontsize=12)\n",
    "    plt.ylabel(\"Acoustic Index\", fontsize=12)\n",
    "    plt.title(\"Correlation of Acoustic Indices with Avian Diversity (Shannon)\", fontsize=16)\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe087fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.2: Identify Best Acoustic Indices via Correlation Plot with simpson ---\n",
    "# This plot replicates the style from the research paper, showing which indices are most strongly\n",
    "# correlated with avian biodiversity (Simpson Index).\n",
    "\n",
    "def bootstrap_spearman(df, col_x, col_y, n_iterations=1000, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    r_values = []\n",
    "    for _ in range(n_iterations):\n",
    "        sample = df.sample(frac=1, replace=True)\n",
    "        r, _ = spearmanr(sample[col_x], sample[col_y])\n",
    "        r_values.append(r)\n",
    "    r_values = np.array(r_values)\n",
    "    return np.mean(r_values), np.percentile(r_values, 2.5), np.percentile(r_values, 97.5)\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    indices_to_test = ['ADI', 'ACI', 'AEI', 'NDSI', 'MFC', 'CLS']\n",
    "    corr_results = []\n",
    "    for index in indices_to_test:\n",
    "        mean_r, ci_lower, ci_upper = bootstrap_spearman(combined_indices_df, index, 'Simpson')\n",
    "        corr_results.append({'Index': index, 'Mean_r': mean_r, 'CI_lower': ci_lower, 'CI_upper': ci_upper})\n",
    "    \n",
    "    df_corr_results = pd.DataFrame(corr_results).sort_values('Mean_r')\n",
    "\n",
    "    # Plotting the correlation graph\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.errorbar(df_corr_results['Mean_r'], df_corr_results['Index'],\n",
    "                 xerr=[df_corr_results['Mean_r'] - df_corr_results['CI_lower'], \n",
    "                       df_corr_results['CI_upper'] - df_corr_results['Mean_r']],\n",
    "                 fmt='o', color='darkslateblue', capsize=5, elinewidth=2, markeredgewidth=2)\n",
    "\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.xlabel(\"Spearman's Rank Correlation (rs) with Simpson Index\", fontsize=12)\n",
    "    plt.ylabel(\"Acoustic Index\", fontsize=12)\n",
    "    plt.title(\"Correlation of Acoustic Indices with Avian Diversity (Simpson)\", fontsize=16)\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_boxplot_cell",
   "metadata": {
    "tags": [
     "new-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Step 5.3: Plotting Comparative Box Plots ---\n",
    "# Based on the correlation plot above, we select the most informative indices\n",
    "# and compare their distributions across the different monitoring spots.\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    # Select the indices with the highest positive correlations from the chart above, plus Shannon.\n",
    "    plot_indices = ['Shannon', 'CLS', 'ACI', 'MFC'] \n",
    "    \n",
    "    print(\"\\nPlotting box plots for the most effective indices...\")\n",
    "    \n",
    "    for index in plot_indices:\n",
    "        if index in combined_indices_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(data=combined_indices_df, x='Spot', y=index, order=sorted(combined_indices_df['Spot'].unique()))\n",
    "            plt.title(f'Distribution of {index} Across Monitoring Spots', fontsize=16)\n",
    "            plt.xlabel('Monitoring Spot', fontsize=12)\n",
    "            plt.ylabel(f'{index} Value', fontsize=12)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09e13b",
   "metadata": {},
   "source": [
    "### Part 6: Regression Analysis\n",
    "This section runs multiple types of regression to check prediction accuracy for Shannon's and Simpson's Index.\n",
    "- **Multiple Linear Regression:** Uses all the indices to see if a linear regression can be fit, providing coefficients\n",
    "- **Random Forest Regressor:** Applies a random forest model to capture non-linear relationships\n",
    "- **Gradient Boost Regressor:** Uses the gradient boosting algorithm for potentially better performance on complex data\n",
    "\n",
    "\n",
    "strong overlaps of code chunks with graphs.ipynb, data generated from there to be used here *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bff204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7.1: Multiple Linear Regression ---\n",
    "# This model attempts to predict the Shannon Index using all available acoustic indices as features.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    features = ['ADI', 'ACI', 'AEI', 'NDSI', 'MFC', 'CLS']\n",
    "    model_df = combined_indices_df.dropna(subset=['Shannon'] + features)\n",
    "    \n",
    "    if len(model_df) > len(features):\n",
    "        X = model_df[features].values\n",
    "        y = model_df[\"Shannon\"].values\n",
    "\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        y_pred = reg.predict(X)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "        print(f\"Multiple Linear Regression using all indices to predict Shannon Index:\")\n",
    "        print(f\"Model R² = {r2:.4f}\")\n",
    "        \n",
    "        # Display coefficients to see feature importance\n",
    "        coef_df = pd.DataFrame(reg.coef_, features, columns=['Coefficient'])\n",
    "        print(\"\\nFeature Coefficients:\")\n",
    "        print(coef_df)\n",
    "    else:\n",
    "        print(\"Not enough data for Linear Regression model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: Graphs.ipynb\n",
    "# Replace the Linear Regression cell (id=\"470513ce\") with this\n",
    "\n",
    "# --- Step 7.1: Advanced Modeling with Random Forest ---\n",
    "# EXPLANATION: As the linear model showed, the relationship is not linear and the indices are correlated.\n",
    "# A Random Forest model is much better suited for this type of data.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    # Select a more robust set of features, removing the redundant AEI\n",
    "    features = ['ADI', 'ACI', 'NDSI', 'MFC', 'CLS']\n",
    "    target = 'Shannon'\n",
    "    \n",
    "    model_df = combined_indices_df.dropna(subset=[target] + features)\n",
    "    \n",
    "    if len(model_df) > 10:\n",
    "        X = model_df[features]\n",
    "        y = model_df[target]\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred_train = rf_model.predict(X_train)\n",
    "        y_pred_test = rf_model.predict(X_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        print(\"Random Forest Regression to predict Shannon Index:\")\n",
    "        print(f\"Train R² = {train_r2:.4f}\")\n",
    "        print(f\"Test R²  = {test_r2:.4f}\")\n",
    "\n",
    "        # Display feature importances\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': rf_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\\\nFeature Importances:\")\n",
    "        print(importance_df)\n",
    "    else:\n",
    "        print(\"Not enough data for Random Forest model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67280316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: Graphs.ipynb\n",
    "# In the cell with id=\"replaced_shannon_cell\" (Step 6.1)\n",
    "# Replace the avg_indices_df calculation with this more detailed one.\n",
    "\n",
    "try:\n",
    "    # indices_df = pd.read_csv(\"newindices.csv\")\n",
    "    # indices_df.rename(columns={'Filename': 'filename'}, inplace=True)\n",
    "    \n",
    "    # --- FEATURE ENGINEERING: Create a richer set of features ---\n",
    "    # Instead of just the mean, calculate multiple stats for each index per file.\n",
    "    avg_indices_df = indices_df.groupby('filename').agg(\n",
    "        ADI_mean=('ADI', 'mean'), ADI_std=('ADI', 'std'),\n",
    "        ACI_mean=('ACI', 'mean'), ACI_std=('ACI', 'std'),\n",
    "        AEI_mean=('AEI', 'mean'), AEI_std=('AEI', 'std'),\n",
    "        NDSI_mean=('NDSI', 'mean'), NDSI_std=('NDSI', 'std'),\n",
    "        MFC_mean=('MFC', 'mean'), MFC_std=('MFC', 'std'),\n",
    "        CLS_mean=('CLS', 'mean'), CLS_std=('CLS', 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Fill any potential NaN values that result from std dev of a single segment\n",
    "    avg_indices_df.fillna(0, inplace=True)\n",
    "\n",
    "    combined_indices_df = pd.merge(shannon_df, avg_indices_df, on='filename', how='inner')\n",
    "    print(\"Successfully merged Shannon and ENHANCED Acoustic Indices.\")\n",
    "    display(combined_indices_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\\\nWARNING: 'newindices.csv' not found. Analysis cannot proceed.\")\n",
    "    combined_indices_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6030197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: Graphs.ipynb\n",
    "# In the Random Forest cell (the last code cell)\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    # Use the new, richer feature set\n",
    "    features = [\n",
    "        'ADI_mean', 'ADI_std', 'ACI_mean', 'ACI_std', \n",
    "        'AEI_mean', 'AEI_std', 'NDSI_mean', 'NDSI_std',\n",
    "        'MFC_mean', 'MFC_std', 'CLS_mean', 'CLS_std'\n",
    "    ]\n",
    "    target = 'Shannon'\n",
    "    \n",
    "    # ... rest of the model training code remains the same ...\n",
    "    model_df = combined_indices_df.dropna(subset=[target] + features)\n",
    "    \n",
    "    if len(model_df) > 10:\n",
    "        X = model_df[features]\n",
    "        y = model_df[target]\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred_train = rf_model.predict(X_train)\n",
    "        y_pred_test = rf_model.predict(X_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        print(\"Random Forest Regression to predict Shannon Index:\")\n",
    "        print(f\"Train R² = {train_r2:.4f}\")\n",
    "        print(f\"Test R²  = {test_r2:.4f}\")\n",
    "\n",
    "        # Display feature importances\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': rf_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\\\nFeature Importances:\")\n",
    "        print(importance_df)\n",
    "    else:\n",
    "        print(\"Not enough data for Random Forest model.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: Graphs.ipynb\n",
    "# Replace the final Random Forest cell with this complete block\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "    # Use the enhanced feature set from Strategy 1\n",
    "    features = [\n",
    "        'ADI_mean', 'ADI_std', 'ACI_mean', 'ACI_std', \n",
    "        'AEI_mean', 'AEI_std', 'NDSI_mean', 'NDSI_std',\n",
    "        'MFC_mean', 'MFC_std', 'CLS_mean', 'CLS_std'\n",
    "    ]\n",
    "    target = 'Shannon'\n",
    "    \n",
    "    model_df = combined_indices_df.dropna(subset=[target] + features)\n",
    "    \n",
    "    if len(model_df) > 20: # Need enough data for grid search\n",
    "        X = model_df[features]\n",
    "        y = model_df[target]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # --- Hyperparameter Tuning with GridSearchCV ---\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None], # Controls tree depth to prevent overfitting\n",
    "            'min_samples_leaf': [3, 5, 10],   # Prevents model from fitting to noise\n",
    "            'max_features': ['sqrt', 'log2']  # Introduces more randomness\n",
    "        }\n",
    "\n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Grid search will find the best combination of parameters using cross-validation\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, \n",
    "                                   scoring='r2', n_jobs=-1, verbose=1)\n",
    "        \n",
    "        print(\"Running GridSearchCV to find the best model parameters...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluate the BEST model\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        print(\"\\\\nOptimized Random Forest Regression Results:\")\n",
    "        print(f\"Train R² = {train_r2:.4f}\")\n",
    "        print(f\"Test R²  = {test_r2:.4f}\")\n",
    "\n",
    "        # Display feature importances from the best model\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\\\nFeature Importances:\")\n",
    "        print(importance_df)\n",
    "    else:\n",
    "        print(\"Not enough data for robust model training and tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde35e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of swapping the model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# In file: Graphs.ipynb\n",
    "# Replace the final Random Forest cell with this complete block\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if not combined_indices_df.empty:\n",
    "\n",
    "    \n",
    "    # Use the enhanced feature set from Strategy 1\n",
    "    features = [\n",
    "        'ADI_mean', 'ADI_std', 'ACI_mean', 'ACI_std', \n",
    "        'AEI_mean', 'AEI_std', 'NDSI_mean', 'NDSI_std',\n",
    "        'MFC_mean', 'MFC_std', 'CLS_mean', 'CLS_std'\n",
    "    ]\n",
    "    target = 'Shannon'\n",
    "    \n",
    "    model_df = combined_indices_df.dropna(subset=[target] + features)\n",
    "    \n",
    "    if len(model_df) > 20: # Need enough data for grid search\n",
    "        X = model_df[features]\n",
    "        y = model_df[target]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # --- Hyperparameter Tuning with GridSearchCV ---\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None], # Controls tree depth to prevent overfitting\n",
    "            'min_samples_leaf': [3, 5, 10],   # Prevents model from fitting to noise\n",
    "            'max_features': ['sqrt', 'log2']  # Introduces more randomness\n",
    "        }\n",
    "\n",
    "\n",
    "        gb_model = GradientBoostingRegressor(random_state=42)\n",
    "        param_grid_gb = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "        grid_search_gb = GridSearchCV(estimator=gb_model, param_grid=param_grid_gb, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "        \n",
    "        print(\"Running GridSearchCV to find the best model parameters...\")\n",
    "        grid_search_gb.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best parameters found: {grid_search_gb.best_params_}\")\n",
    "        \n",
    "        best_model = grid_search_gb.best_estimator_\n",
    "\n",
    "        # Evaluate the BEST model\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        print(\"\\\\nOptimized Random Forest Regression Results:\")\n",
    "        print(f\"Train R² = {train_r2:.4f}\")\n",
    "        print(f\"Test R²  = {test_r2:.4f}\")\n",
    "\n",
    "        # Display feature importances from the best model\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\\\nFeature Importances:\")\n",
    "        print(importance_df)\n",
    "    else:\n",
    "        print(\"Not enough data for robust model training and tuning.\")\n",
    "\n",
    "# ... (inside the if not combined_indices_df.empty block) ...\n",
    "\n",
    "# ... etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
